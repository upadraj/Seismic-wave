{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torchmetrics\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Example for RGB images\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load(\"./data/balanced/input_data.npy\")\n",
    "labels = np.load(\"./data/balanced/label_data.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tensor = torch.from_numpy(x)\n",
    "x_t = x_tensor.view(-1, 3, 49, 49)\n",
    "inputs = x_t.float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Mapping: {'Lg': 0, 'P': 1, 'Pg': 2, 'Pn': 3, 'S': 4, 'Sn': 5}\n"
     ]
    }
   ],
   "source": [
    "unique_labels = np.unique(labels)\n",
    "label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "print(\"Label Mapping:\", label_mapping)\n",
    "integer_labels = np.array([label_mapping[label] for label in labels])\n",
    "targets = torch.from_numpy(integer_labels).long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = TensorDataset(inputs, targets)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_min = inputs.min()\n",
    "# x_max = inputs.max()\n",
    "# print((x_min, x_max))\n",
    "\n",
    "# inputs = 2 * (inputs - x_min) / (x_max - x_min) -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=2, padding=1)\n",
    "        # self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=32, out_channels=64, kernel_size=2, padding=1\n",
    "        )\n",
    "        # self.bn2 = nn.BatchNorm2d(64)  # Add BatchNorm after the second conv layer\n",
    "\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=64, out_channels=128, kernel_size=2, padding=1\n",
    "        )\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        # self.bn3 = nn.BatchNorm2d(128)  # Add BatchNorm after the third conv layer\n",
    "\n",
    "        # self.dropout = nn.Dropout(p=0.25)\n",
    "\n",
    "        self.fc1 = nn.Linear(128 * 7 * 7, 256)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn_fc2 = nn.BatchNorm1d(128)\n",
    "        self.fc3 = nn.Linear(128, 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.leaky_relu((self.conv1(x))))  # Conv1 -> BN -> ReLU -> Pool\n",
    "        x = self.pool(F.tanh((self.conv2(x))))  # Conv2 -> BN -> Tanh -> Pool\n",
    "        x = self.pool(F.tanh((self.conv3(x))))  # Conv3 -> BN -> Tanh -> Pool\n",
    "        x = x.view(-1, 128 * 7 * 7)\n",
    "        x = F.tanh(self.bn_fc1(self.fc1(x)))  # FC1 -> BN -> Tanh\n",
    "        x = F.tanh(self.bn_fc2(self.fc2(x)))  # FC2 -> BN -> Tanh\n",
    "        x = self.fc3(x)  # FC3 (no activation here because this is the final output)\n",
    "\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Cross Entropy Loss: 1.7089,Learningrate:  0.00000800\n",
      "Epoch 2/50, Cross Entropy Loss: 1.6344,Learningrate:  0.00001520\n",
      "Epoch 3/50, Cross Entropy Loss: 1.5959,Learningrate:  0.00002240\n",
      "Epoch 4/50, Cross Entropy Loss: 1.5635,Learningrate:  0.00002960\n",
      "Epoch 5/50, Cross Entropy Loss: 1.5374,Learningrate:  0.00003680\n",
      "Epoch 6/50, Cross Entropy Loss: 1.5133,Learningrate:  0.00004400\n",
      "Epoch 7/50, Cross Entropy Loss: 1.4940,Learningrate:  0.00005120\n",
      "Epoch 8/50, Cross Entropy Loss: 1.4790,Learningrate:  0.00005840\n",
      "Epoch 9/50, Cross Entropy Loss: 1.4634,Learningrate:  0.00006560\n",
      "Epoch 10/50, Cross Entropy Loss: 1.4466,Learningrate:  0.00007280\n",
      "Epoch 11/50, Cross Entropy Loss: 1.4324,Learningrate:  0.00007280\n",
      "Epoch 12/50, Cross Entropy Loss: 1.4198,Learningrate:  0.00007280\n",
      "Epoch 13/50, Cross Entropy Loss: 1.4065,Learningrate:  0.00007280\n",
      "Epoch 14/50, Cross Entropy Loss: 1.3957,Learningrate:  0.00007280\n",
      "Epoch 15/50, Cross Entropy Loss: 1.3863,Learningrate:  0.00007280\n",
      "Epoch 16/50, Cross Entropy Loss: 1.3732,Learningrate:  0.00007280\n",
      "Epoch 17/50, Cross Entropy Loss: 1.3613,Learningrate:  0.00007280\n",
      "Epoch 18/50, Cross Entropy Loss: 1.3516,Learningrate:  0.00007280\n",
      "Epoch 19/50, Cross Entropy Loss: 1.3430,Learningrate:  0.00007280\n",
      "Epoch 20/50, Cross Entropy Loss: 1.3334,Learningrate:  0.00000728\n",
      "Epoch 21/50, Cross Entropy Loss: 1.3211,Learningrate:  0.00000728\n",
      "Epoch 22/50, Cross Entropy Loss: 1.3162,Learningrate:  0.00000728\n",
      "Epoch 23/50, Cross Entropy Loss: 1.3071,Learningrate:  0.00000728\n",
      "Epoch 24/50, Cross Entropy Loss: 1.2991,Learningrate:  0.00000728\n",
      "Epoch 25/50, Cross Entropy Loss: 1.2894,Learningrate:  0.00000728\n",
      "Epoch 26/50, Cross Entropy Loss: 1.2811,Learningrate:  0.00000728\n",
      "Epoch 27/50, Cross Entropy Loss: 1.2739,Learningrate:  0.00000728\n",
      "Epoch 28/50, Cross Entropy Loss: 1.2632,Learningrate:  0.00000728\n",
      "Epoch 29/50, Cross Entropy Loss: 1.2588,Learningrate:  0.00000728\n",
      "Epoch 30/50, Cross Entropy Loss: 1.2500,Learningrate:  0.00000073\n",
      "Epoch 31/50, Cross Entropy Loss: 1.2416,Learningrate:  0.00000073\n",
      "Epoch 32/50, Cross Entropy Loss: 1.2375,Learningrate:  0.00000073\n",
      "Epoch 33/50, Cross Entropy Loss: 1.2247,Learningrate:  0.00000073\n",
      "Epoch 34/50, Cross Entropy Loss: 1.2208,Learningrate:  0.00000073\n",
      "Epoch 35/50, Cross Entropy Loss: 1.2115,Learningrate:  0.00000073\n",
      "Epoch 36/50, Cross Entropy Loss: 1.2034,Learningrate:  0.00000073\n",
      "Epoch 37/50, Cross Entropy Loss: 1.1999,Learningrate:  0.00000073\n",
      "Epoch 38/50, Cross Entropy Loss: 1.1878,Learningrate:  0.00000073\n",
      "Epoch 39/50, Cross Entropy Loss: 1.1792,Learningrate:  0.00000073\n",
      "Epoch 40/50, Cross Entropy Loss: 1.1748,Learningrate:  0.00000007\n",
      "Epoch 41/50, Cross Entropy Loss: 1.1685,Learningrate:  0.00000007\n",
      "Epoch 42/50, Cross Entropy Loss: 1.1638,Learningrate:  0.00000007\n",
      "Epoch 43/50, Cross Entropy Loss: 1.1539,Learningrate:  0.00000007\n",
      "Epoch 44/50, Cross Entropy Loss: 1.1482,Learningrate:  0.00000007\n",
      "Epoch 45/50, Cross Entropy Loss: 1.1411,Learningrate:  0.00000007\n",
      "Epoch 46/50, Cross Entropy Loss: 1.1346,Learningrate:  0.00000007\n",
      "Epoch 47/50, Cross Entropy Loss: 1.1276,Learningrate:  0.00000007\n",
      "Epoch 48/50, Cross Entropy Loss: 1.1210,Learningrate:  0.00000007\n",
      "Epoch 49/50, Cross Entropy Loss: 1.1136,Learningrate:  0.00000007\n",
      "Epoch 50/50, Cross Entropy Loss: 1.1066,Learningrate:  0.00000001\n"
     ]
    }
   ],
   "source": [
    "model = CNN()\n",
    "model = model.to(device=device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Rprop(model.parameters(), lr=0.0008)\n",
    "\n",
    "# Warmup settings\n",
    "initial_lr = 0.000008  # Starting learning rate for warmup\n",
    "base_lr = 0.00008  # Target learning rate after warmup\n",
    "warmup_steps = 10  # Number of warmup steps (epochs)\n",
    "epochs = 50\n",
    "\n",
    "# Learning rate scheduler after warmup\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Inputs and targets to device\n",
    "inputs = inputs.to(device=device)\n",
    "targets = targets.to(device=device)\n",
    "\n",
    "# Training loop with warmup steps and learning rate scheduler\n",
    "for epoch in range(epochs):\n",
    "    p_lr = \"\"\n",
    "    # Warmup phase\n",
    "    if epoch < warmup_steps:\n",
    "        # Linear warmup\n",
    "        lr = initial_lr + (base_lr - initial_lr) * (epoch / warmup_steps)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "        # print(f\"Warmup Epoch {epoch+1}/{warmup_steps}, Learning Rate: {lr:.8f}\")\n",
    "    else:\n",
    "        # After warmup, allow the scheduler to control the learning rate\n",
    "        scheduler.step()  # Step the scheduler after each epoch\n",
    "        for param_group in optimizer.param_groups:\n",
    "            lr = param_group[\"lr\"]  # Get the current learning rate from the scheduler\n",
    "    # print(f\"Epoch {epoch+1}/{epochs}, Learning Rate: {lr:.8f}\")\n",
    "\n",
    "    # Training loop for each batch\n",
    "    batch_loss = 0.0\n",
    "    for batch_inputs, batch_targets in dataloader:\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_inputs)\n",
    "        loss = criterion(outputs, batch_targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_loss += loss.item()\n",
    "\n",
    "    # Calculate average loss for the epoch\n",
    "    avg_loss = batch_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Cross Entropy Loss: {avg_loss:.4f},Learningrate: {lr: 0.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Mapping: {'Lg': 0, 'P': 1, 'Pg': 2, 'Pn': 3, 'S': 4, 'Sn': 5}\n"
     ]
    }
   ],
   "source": [
    "x = np.load(\"./data/balanced/test_data.npy\")\n",
    "labels = np.load(\"./data/balanced/test_label_data.npy\", allow_pickle=True)\n",
    "\n",
    "x_tensor = torch.from_numpy(x)\n",
    "x_t = x_tensor.view(-1, 3, 49, 49)\n",
    "inputs = x_t.float()\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "unique_labels = np.unique(labels)\n",
    "label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "print(\"Label Mapping:\", label_mapping)\n",
    "integer_labels = np.array([label_mapping[label] for label in labels])\n",
    "targets = torch.from_numpy(integer_labels)\n",
    "targets = targets.long()\n",
    "targets = targets.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 1, 0,  ..., 5, 0, 5], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "y_pred = model(inputs)\n",
    "probabilities = F.softmax(y_pred, dim=1)\n",
    "predicted_class = torch.argmax(probabilities, dim=1)\n",
    "\n",
    "predicted_class = predicted_class.to(device=device)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4488, device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_metric  = torchmetrics.Accuracy(task=\"multiclass\", num_classes=6).to(device=device)\n",
    "accuracy = accuracy_metric(predicted_class, targets)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Mapping: {'Lg': 0, 'P': 1, 'Pg': 2, 'Pn': 3, 'S': 4, 'Sn': 5}\n",
      "Accuracy on the test data: 44.57%\n"
     ]
    }
   ],
   "source": [
    "test_data = np.load(\"./data/balanced/test_data.npy\")\n",
    "test_labels = np.load(\"./data/balanced/test_label_data.npy\", allow_pickle=True)\n",
    "\n",
    "test_data_tensor = torch.from_numpy(test_data)\n",
    "test_data_tensor = test_data_tensor.view(-1, 3, 49, 49)  # Reshape to [batch_size, 3, 49, 49]\n",
    "test_inputs = test_data_tensor.float()\n",
    "test_inputs = test_inputs.to(device)\n",
    "\n",
    "unique_labels = np.unique(test_labels)\n",
    "label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "print(\"Label Mapping:\", label_mapping)\n",
    "integer_test_labels = np.array([label_mapping[label] for label in test_labels])\n",
    "\n",
    "test_targets = torch.from_numpy(integer_test_labels).long()\n",
    "test_targets = test_targets.to(device)\n",
    "\n",
    "batch_size = 32  # Define the batch size\n",
    "test_dataset = TensorDataset(test_inputs, test_targets)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=len(unique_labels)).to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_test_inputs, batch_test_targets in test_dataloader:\n",
    "        outputs = model(batch_test_inputs)\n",
    "\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        accuracy.update(predictions, batch_test_targets)\n",
    "\n",
    "final_accuracy = accuracy.compute().item()\n",
    "print(f\"Accuracy on the test data: {final_accuracy * 100:.2f}%\")\n",
    "\n",
    "accuracy.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a1_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
