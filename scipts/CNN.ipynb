{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torchmetrics\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = np.load(\"../data/processed/balance_16k/model_input/train_features.npy\")\n",
    "train_labels = np.load(\"../data/processed/balance_16k/model_input/train_labels.npy\")\n",
    "\n",
    "test_features = np.load(\"../data/processed/balance_16k/model_input/test_features.npy\")\n",
    "test_labels = np.load(\"../data/processed/balance_16k/model_input/test_labels.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = torch.from_numpy(train_features).float()\n",
    "train_features = train_features.to(device)\n",
    "train_labels = torch.from_numpy(train_labels)\n",
    "train_labels = train_labels.to(device)\n",
    "\n",
    "test_features = torch.from_numpy(test_features).float()\n",
    "test_features = test_features.to(device)\n",
    "test_labels = torch.from_numpy(test_labels)\n",
    "test_labels = test_labels.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train features: torch.Size([14672, 3, 50, 50]), Train label: torch.Size([14672]) \n",
      "Test features: torch.Size([1631, 3, 50, 50]), Test label: torch.Size([1631]) \n",
      "(tensor(0., device='cuda:0'), tensor(13373320., device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train features: {train_features.shape}, Train label: {train_labels.shape} \")\n",
    "print(f\"Test features: {test_features.shape}, Test label: {test_labels.shape} \")\n",
    "train_min, train_max = train_features.min(), train_features.max()\n",
    "print((train_min, train_max))\n",
    "\n",
    "# inputs = 2 * (inputs - x_min) / (x_max - x_min) -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = TensorDataset(train_features, train_labels)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=2, padding=1)\n",
    "        # self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=32, out_channels=64, kernel_size=2, padding=1\n",
    "        )\n",
    "        # self.bn2 = nn.BatchNorm2d(64) \n",
    "\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=64, out_channels=128, kernel_size=2, padding=1\n",
    "        )\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        # self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "        # self.dropout = nn.Dropout(p=0.25)\n",
    "\n",
    "        self.fc1 = nn.Linear(128 * 7 * 7, 256)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn_fc2 = nn.BatchNorm1d(128)\n",
    "        self.fc3 = nn.Linear(128, 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.leaky_relu(self.conv1(x)))  # Conv1 -> -> leaky_relu -> Pool\n",
    "        x = self.pool(F.tanh((self.conv2(x))))  # Conv2 -> BN -> Tanh -> Pool\n",
    "        x = self.pool(F.tanh((self.conv3(x))))  # Conv3 -> BN -> Tanh -> Pool\n",
    "        x = x.view(-1, 128 * 7 * 7)\n",
    "        x = F.tanh(self.bn_fc1(self.fc1(x)))  # FC1 -> BN -> Tanh\n",
    "        x = F.tanh(self.bn_fc2(self.fc2(x)))  # FC2 -> BN -> Tanh\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "model = model.to(device=device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Rprop(model.parameters(), lr=0.0008)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "initial_lr = 0.0008  \n",
    "base_lr = 0.0008  \n",
    "warmup_steps = 10 \n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model=model,\n",
    "    dataloader=dataloader,\n",
    "    device=device,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    epochs=epochs,\n",
    "    warmup_steps=warmup_steps,\n",
    "    initial_lr=initial_lr,\n",
    "    base_lr=base_lr,\n",
    "):\n",
    "    model = model.to(device=device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if epoch < warmup_steps:\n",
    "            lr = initial_lr + (base_lr - initial_lr) * (epoch / warmup_steps)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group[\"lr\"] = lr\n",
    "        else:\n",
    "            scheduler.step()\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "        batch_loss = 0.0\n",
    "        model.train()\n",
    "        for batch_inputs, batch_targets in dataloader:\n",
    "            batch_inputs = batch_inputs.to(device)\n",
    "            batch_targets = batch_targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_inputs)\n",
    "            loss = criterion(outputs, batch_targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_loss += loss.item()\n",
    "\n",
    "        avg_loss = batch_loss / len(dataloader)\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{epochs}, Cross Entropy Loss: {avg_loss:.4f}, Learning Rate: {lr:.6f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Cross Entropy Loss: 1.2093, Learning Rate: 0.000800\n",
      "Epoch 2/20, Cross Entropy Loss: 1.1379, Learning Rate: 0.000800\n",
      "Epoch 3/20, Cross Entropy Loss: 1.1089, Learning Rate: 0.000800\n",
      "Epoch 4/20, Cross Entropy Loss: 1.0801, Learning Rate: 0.000800\n",
      "Epoch 5/20, Cross Entropy Loss: 1.0576, Learning Rate: 0.000800\n",
      "Epoch 6/20, Cross Entropy Loss: 1.0380, Learning Rate: 0.000800\n",
      "Epoch 7/20, Cross Entropy Loss: 1.0149, Learning Rate: 0.000800\n",
      "Epoch 8/20, Cross Entropy Loss: 0.9917, Learning Rate: 0.000800\n",
      "Epoch 9/20, Cross Entropy Loss: 0.9744, Learning Rate: 0.000800\n",
      "Epoch 10/20, Cross Entropy Loss: 0.9564, Learning Rate: 0.000800\n",
      "Epoch 11/20, Cross Entropy Loss: 0.9450, Learning Rate: 0.000800\n",
      "Epoch 12/20, Cross Entropy Loss: 0.9288, Learning Rate: 0.000800\n",
      "Epoch 13/20, Cross Entropy Loss: 0.9128, Learning Rate: 0.000800\n",
      "Epoch 14/20, Cross Entropy Loss: 0.9023, Learning Rate: 0.000800\n",
      "Epoch 15/20, Cross Entropy Loss: 0.8898, Learning Rate: 0.000800\n",
      "Epoch 16/20, Cross Entropy Loss: 0.8792, Learning Rate: 0.000800\n",
      "Epoch 17/20, Cross Entropy Loss: 0.8656, Learning Rate: 0.000800\n",
      "Epoch 18/20, Cross Entropy Loss: 0.8546, Learning Rate: 0.000800\n",
      "Epoch 19/20, Cross Entropy Loss: 0.8454, Learning Rate: 0.000800\n",
      "Epoch 20/20, Cross Entropy Loss: 0.8345, Learning Rate: 0.000080\n"
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test data: 67.81%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "test_dataset = TensorDataset(test_features, test_labels)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=6).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_test_inputs, batch_test_targets in test_dataloader:\n",
    "        outputs = model(batch_test_inputs)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        accuracy.update(predictions, batch_test_targets)\n",
    "\n",
    "final_accuracy = accuracy.compute().item()\n",
    "print(f\"Accuracy on the test data: {final_accuracy * 100:.2f}%\")\n",
    "\n",
    "accuracy.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a1_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
