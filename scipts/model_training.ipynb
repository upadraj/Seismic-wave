{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T17:47:34.475207Z",
     "start_time": "2024-09-08T17:47:34.471241Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import torchmetrics\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(r'C:\\Users\\pradh\\OneDrive - University of New Mexico\\Desktop\\Fall 2024\\CS521- Data Mining\\Data_mining')))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from models.cnn import CNN\n",
    "from models.resnet import ResNet, PretrainedResNet\n",
    "from models.loss import FocalLoss\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.calibration import calibration_curve\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from utils.data_utils import count_unique_colum_and_vlaues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T17:47:34.649218Z",
     "start_time": "2024-09-08T17:47:34.646285Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T17:47:35.089787Z",
     "start_time": "2024-09-08T17:47:34.821451Z"
    }
   },
   "outputs": [],
   "source": [
    "train_features = np.load(\"../data/processed/balance_16k/model_input/train_features.npy\", allow_pickle=True)\n",
    "train_labels = np.load(\"../data/processed/balance_16k/model_input/train_labels.npy\", allow_pickle=True)\n",
    "\n",
    "test_features = np.load(\"../data/processed/balance_16k/model_input/test_features.npy\", allow_pickle=True)\n",
    "test_labels = np.load(\"../data/processed/balance_16k/model_input/test_labels.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T17:47:35.346728Z",
     "start_time": "2024-09-08T17:47:35.089787Z"
    }
   },
   "outputs": [],
   "source": [
    "train_features = torch.from_numpy(train_features).float()\n",
    "train_features = train_features.to(device)\n",
    "train_labels = torch.from_numpy(train_labels)\n",
    "train_labels = train_labels.to(device)\n",
    "\n",
    "test_features = torch.from_numpy(test_features).float()\n",
    "test_features = test_features.to(device)\n",
    "test_labels = torch.from_numpy(test_labels)\n",
    "test_labels = test_labels.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T17:47:35.464124Z",
     "start_time": "2024-09-08T17:47:35.346728Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train features: torch.Size([15854, 3, 50, 50]), Train label: torch.Size([18218]) \n",
      "Test features: torch.Size([1631, 3, 50, 50]), Test label: torch.Size([1631]) \n",
      "(tensor(0., device='cuda:0'), tensor(17645068., device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train features: {train_features.shape}, Train label: {train_labels.shape} \")\n",
    "print(f\"Test features: {test_features.shape}, Test label: {test_labels.shape} \")\n",
    "train_min, train_max = train_features.min(), train_features.max()\n",
    "print((train_min, train_max))\n",
    "\n",
    "# inputs = 2 * (inputs - x_min) / (x_max - x_min) -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T17:47:35.468901Z",
     "start_time": "2024-09-08T17:47:35.465127Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2699\n",
      "1 2695\n",
      "2 2707\n",
      "3 2708\n",
      "4 4728\n",
      "5 2681\n"
     ]
    }
   ],
   "source": [
    "unique_value, count = count_unique_colum_and_vlaues(train_labels.to(device='cpu'))\n",
    "for x in range(len(count)):\n",
    "    print(unique_value[x], count[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T17:47:35.726894Z",
     "start_time": "2024-09-08T17:47:35.540071Z"
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Size mismatch between tensors",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m32\u001B[39m\n\u001B[1;32m----> 2\u001B[0m dataset \u001B[38;5;241m=\u001B[39m \u001B[43mTensorDataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_features\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_labels\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m dataloader \u001B[38;5;241m=\u001B[39m DataLoader(dataset, batch_size\u001B[38;5;241m=\u001B[39mbatch_size, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m      5\u001B[0m test_dataset \u001B[38;5;241m=\u001B[39m TensorDataset(test_features, test_labels)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:205\u001B[0m, in \u001B[0;36mTensorDataset.__init__\u001B[1;34m(self, *tensors)\u001B[0m\n\u001B[0;32m    204\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mtensors: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 205\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mall\u001B[39m(\n\u001B[0;32m    206\u001B[0m         tensors[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m) \u001B[38;5;241m==\u001B[39m tensor\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m tensor \u001B[38;5;129;01min\u001B[39;00m tensors\n\u001B[0;32m    207\u001B[0m     ), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSize mismatch between tensors\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    208\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtensors \u001B[38;5;241m=\u001B[39m tensors\n",
      "\u001B[1;31mAssertionError\u001B[0m: Size mismatch between tensors"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "dataset = TensorDataset(train_features, train_labels)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_features, test_labels)\n",
    "test_dataloader = DataLoader(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T18:20:44.225238Z",
     "start_time": "2024-09-08T18:20:43.469117Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "initial_lr = 1e-4\n",
    "base_lr = 1e-4\n",
    "warmup_steps = 10\n",
    "epochs = 30\n",
    "\n",
    "model = CNN()\n",
    "model = model.to(device=device)\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "# criterion = FocalLoss()\n",
    "optimizer = optim.Rprop(model.parameters(), lr=initial_lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T18:20:44.228750Z",
     "start_time": "2024-09-08T18:20:44.225238Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "CNN(\n  (conv1): Conv2d(3, 128, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n  (conv2): Conv2d(128, 256, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n  (conv3): Conv2d(256, 512, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (fc1): Linear(in_features=373248, out_features=512, bias=True)\n  (fc2): Linear(in_features=512, out_features=256, bias=True)\n  (fc3): Linear(in_features=256, out_features=6, bias=True)\n)"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T18:20:44.232828Z",
     "start_time": "2024-09-08T18:20:44.228750Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model=model,\n",
    "    dataloader=dataloader,\n",
    "    device=device,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    epochs=epochs,\n",
    "    warmup_steps=warmup_steps,\n",
    "    initial_lr=initial_lr,\n",
    "    base_lr=base_lr,\n",
    "):\n",
    "    model = model.to(device=device)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        if epoch < warmup_steps:\n",
    "            lr = initial_lr + (base_lr - initial_lr) * (epoch / warmup_steps)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group[\"lr\"] = lr\n",
    "        else:\n",
    "            scheduler.step()\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "        batch_loss = 0.0\n",
    "        for batch_inputs, batch_targets in dataloader:\n",
    "            batch_inputs = batch_inputs.to(device)\n",
    "            batch_targets = batch_targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_inputs)\n",
    "            loss = criterion(outputs, batch_targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_loss += loss.item()\n",
    "\n",
    "        avg_loss = batch_loss / len(dataloader)\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{epochs}, Cross Entropy Loss: {avg_loss:.4f}, Learning Rate: {initial_lr:.6f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T20:02:54.714245Z",
     "start_time": "2024-09-08T18:20:44.505145Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Cross Entropy Loss: 1.3321, Learning Rate: 0.000001\n",
      "Epoch 2/30, Cross Entropy Loss: 1.1004, Learning Rate: 0.000001\n",
      "Epoch 3/30, Cross Entropy Loss: 0.9934, Learning Rate: 0.000001\n",
      "Epoch 4/30, Cross Entropy Loss: 0.9177, Learning Rate: 0.000001\n",
      "Epoch 5/30, Cross Entropy Loss: 0.8463, Learning Rate: 0.000001\n",
      "Epoch 6/30, Cross Entropy Loss: 0.7800, Learning Rate: 0.000001\n",
      "Epoch 7/30, Cross Entropy Loss: 0.7158, Learning Rate: 0.000001\n",
      "Epoch 8/30, Cross Entropy Loss: 0.6534, Learning Rate: 0.000001\n",
      "Epoch 9/30, Cross Entropy Loss: 0.5985, Learning Rate: 0.000001\n",
      "Epoch 10/30, Cross Entropy Loss: 0.5561, Learning Rate: 0.000001\n",
      "Epoch 11/30, Cross Entropy Loss: 0.5262, Learning Rate: 0.000001\n",
      "Epoch 12/30, Cross Entropy Loss: 0.5047, Learning Rate: 0.000001\n",
      "Epoch 13/30, Cross Entropy Loss: 0.4872, Learning Rate: 0.000001\n",
      "Epoch 14/30, Cross Entropy Loss: 0.4736, Learning Rate: 0.000001\n",
      "Epoch 15/30, Cross Entropy Loss: 0.4634, Learning Rate: 0.000001\n",
      "Epoch 16/30, Cross Entropy Loss: 0.4557, Learning Rate: 0.000001\n",
      "Epoch 17/30, Cross Entropy Loss: 0.4498, Learning Rate: 0.000001\n",
      "Epoch 18/30, Cross Entropy Loss: 0.4447, Learning Rate: 0.000001\n",
      "Epoch 19/30, Cross Entropy Loss: 0.4416, Learning Rate: 0.000001\n",
      "Epoch 20/30, Cross Entropy Loss: 0.4388, Learning Rate: 0.000001\n",
      "Epoch 21/30, Cross Entropy Loss: 0.4364, Learning Rate: 0.000001\n",
      "Epoch 22/30, Cross Entropy Loss: 0.4342, Learning Rate: 0.000001\n",
      "Epoch 23/30, Cross Entropy Loss: 0.4327, Learning Rate: 0.000001\n",
      "Epoch 24/30, Cross Entropy Loss: 0.4313, Learning Rate: 0.000001\n",
      "Epoch 25/30, Cross Entropy Loss: 0.4304, Learning Rate: 0.000001\n",
      "Epoch 26/30, Cross Entropy Loss: 0.4295, Learning Rate: 0.000001\n",
      "Epoch 27/30, Cross Entropy Loss: 0.4287, Learning Rate: 0.000001\n",
      "Epoch 28/30, Cross Entropy Loss: 0.4280, Learning Rate: 0.000001\n",
      "Epoch 29/30, Cross Entropy Loss: 0.4274, Learning Rate: 0.000001\n",
      "Epoch 30/30, Cross Entropy Loss: 0.4269, Learning Rate: 0.000001\n"
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T21:26:51.638218Z",
     "start_time": "2024-09-08T21:26:46.221748Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Accuracy: 69.65%\n",
      "Accuracy of class 0: 67.77%\n",
      "Accuracy of class 1: 76.07%\n",
      "Accuracy of class 2: 76.11%\n",
      "Accuracy of class 3: 64.73%\n",
      "Accuracy of class 4: 64.46%\n",
      "Accuracy of class 5: 65.83%\n"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy_and_probabilities(model, dataloader, num_classes):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    class_correct = torch.zeros(num_classes)\n",
    "    class_samples = torch.zeros(num_classes)\n",
    "\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "            all_probs.append(probs.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "            total_correct += (predictions == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "            for i in range(len(labels)):\n",
    "                label = labels[i]\n",
    "                if predictions[i] == label:\n",
    "                    class_correct[label] += 1\n",
    "                class_samples[label] += 1\n",
    "\n",
    "    y_prob = torch.cat(all_probs, dim=0)\n",
    "    y_true = torch.cat(all_labels, dim=0)\n",
    "\n",
    "    total_accuracy = total_correct / total_samples * 100\n",
    "    class_accuracy = class_correct / class_samples * 100\n",
    "\n",
    "    return total_accuracy, class_accuracy, y_prob, y_true\n",
    "\n",
    "\n",
    "num_classes = 6\n",
    "total_accuracy, class_accuracy, y_prob, y_true = calculate_accuracy_and_probabilities(\n",
    "    model, dataloader=test_dataloader, num_classes=num_classes\n",
    ")\n",
    "\n",
    "print(f\"Total Accuracy: {total_accuracy:.2f}%\")\n",
    "for i in range(num_classes):\n",
    "    print(f\"Accuracy of class {i}: {class_accuracy[i]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-08T17:47:47.535794Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_calibration_curve(y_true, y_prob, n_bins=10):\n",
    "    y_true_np = y_true.cpu().numpy()\n",
    "    y_prob_np = y_prob.cpu().detach().numpy()\n",
    "\n",
    "    y_prob_max = np.max(y_prob_np, axis=1)\n",
    "\n",
    "    y_pred_max = np.argmax(y_prob_np, axis=1)\n",
    "\n",
    "    prob_true, prob_pred = calibration_curve(\n",
    "        y_true_np == y_pred_max, y_prob_max, n_bins=n_bins\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(prob_pred, prob_true, marker=\"o\", label=\"Calibration curve\")\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", label=\"Perfect calibration (ideal)\")\n",
    "\n",
    "    plt.title(\"Calibration Plot for Multi-Class Model (Max Confidence)\")\n",
    "    plt.xlabel(\"Mean predicted probability\")\n",
    "    plt.ylabel(\"True probability\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    brier_score = brier_score_loss(y_true_np == y_pred_max, y_prob_max)\n",
    "    print(f\"Brier score loss: {brier_score:.4f}\")\n",
    "\n",
    "\n",
    "plot_calibration_curve(y_true, y_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "\n",
    "# test_dataset = TensorDataset(test_features, test_labels)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=6).to(device)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for batch_test_inputs, batch_test_targets in test_dataloader:\n",
    "#         outputs = model(batch_test_inputs)\n",
    "#         predictions = torch.argmax(outputs, dim=1)\n",
    "#         accuracy.update(predictions, batch_test_targets)\n",
    "\n",
    "# final_accuracy = accuracy.compute().item()\n",
    "# print(f\"Accuracy on the test data: {final_accuracy * 100:.2f}%\")\n",
    "\n",
    "# accuracy.reset()\n",
    "\n",
    "\n",
    "# def lr_finder(model, criterion, optimizer, train_loader, init_value=1e-10, final_value=10, beta=0.98):\n",
    "#     num = len(train_loader) - 1\n",
    "#     mult = (final_value / init_value) ** (1/num)\n",
    "#     lr = init_value\n",
    "#     optimizer.param_groups[0]['lr'] = lr\n",
    "#     avg_loss, best_loss = 0.0, 0.0\n",
    "#     losses, log_lrs = [], []\n",
    "\n",
    "#     for i, (inputs, labels) in enumerate(train_loader):\n",
    "#         # Move data to the correct device\n",
    "#         inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "#         # Zero the gradients\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs, labels)\n",
    "\n",
    "#         # Compute the smoothed loss\n",
    "#         avg_loss = beta * avg_loss + (1 - beta) * loss.item()\n",
    "#         smoothed_loss = avg_loss / (1 - beta ** (i + 1))\n",
    "\n",
    "#         # Stop if the loss is exploding\n",
    "#         if i > 1 and smoothed_loss > 4 * best_loss:\n",
    "#             break\n",
    "\n",
    "#         # Record the best loss\n",
    "#         if smoothed_loss < best_loss or i == 0:\n",
    "#             best_loss = smoothed_loss\n",
    "\n",
    "#         # Backward pass and update the weights\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Store the values\n",
    "#         losses.append(smoothed_loss)\n",
    "#         log_lrs.append(lr)\n",
    "\n",
    "#         # Update the learning rate\n",
    "#         lr *= mult\n",
    "#         optimizer.param_groups[0]['lr'] = lr\n",
    "\n",
    "#     # Plot the learning rate vs loss graph\n",
    "#     plt.plot(log_lrs, losses)\n",
    "#     plt.xscale('log')\n",
    "#     plt.xlabel(\"Learning Rate\")\n",
    "#     plt.ylabel(\"Loss\")\n",
    "#     plt.show()\n",
    "\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-8)\n",
    "\n",
    "# lr_finder(model, criterion, optimizer, dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a1_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
