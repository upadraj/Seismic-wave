{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import torchmetrics\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(r'C:\\Users\\incognito\\OneDrive\\Desktop\\data_mining')))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from models.cnn import CNN\n",
    "from torchvision import transforms\n",
    "from models.resnet import ResNet\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from utils.data_utils import count_unique_colum_and_vlaues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = np.load(\"../data/processed/balance_16k/model_input/train_features.npy\")\n",
    "train_labels = np.load(\"../data/processed/balance_16k/model_input/train_labels.npy\")\n",
    "\n",
    "test_features = np.load(\"../data/processed/balance_16k/model_input/test_features.npy\")\n",
    "test_labels = np.load(\"../data/processed/balance_16k/model_input/test_labels.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = torch.from_numpy(train_features).float()\n",
    "train_features = train_features.to(device)\n",
    "train_labels = torch.from_numpy(train_labels)\n",
    "train_labels = train_labels.to(device)\n",
    "\n",
    "test_features = torch.from_numpy(test_features).float()\n",
    "test_features = test_features.to(device)\n",
    "test_labels = torch.from_numpy(test_labels)\n",
    "test_labels = test_labels.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train features: torch.Size([14672, 3, 50, 50]), Train label: torch.Size([14672]) \n",
      "Test features: torch.Size([1631, 3, 50, 50]), Test label: torch.Size([1631]) \n",
      "(tensor(0., device='cuda:0'), tensor(13373320., device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train features: {train_features.shape}, Train label: {train_labels.shape} \")\n",
    "print(f\"Test features: {test_features.shape}, Test label: {test_labels.shape} \")\n",
    "train_min, train_max = train_features.min(), train_features.max()\n",
    "print((train_min, train_max))\n",
    "\n",
    "# inputs = 2 * (inputs - x_min) / (x_max - x_min) -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2713\n",
      "1 2703\n",
      "2 2712\n",
      "3 2676\n",
      "4 1185\n",
      "5 2683\n"
     ]
    }
   ],
   "source": [
    "unique_value, count = count_unique_colum_and_vlaues(train_labels.to(device='cpu'))\n",
    "for x in range(len(count)):\n",
    "    print(unique_value[x], count[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = TensorDataset(train_features, train_labels)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_features, test_labels)\n",
    "test_dataloader = DataLoader(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "# our defined resnet model's performance based best learning rate.\n",
    "# initial_lr = 1e-4\n",
    "# base_lr = 1e-4\n",
    "\n",
    "initial_lr = 1e-4\n",
    "base_lr = 1e-4\n",
    "warmup_steps = 10 \n",
    "epochs = 20\n",
    "\n",
    "model = CNN()\n",
    "model = model.to(device=device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Rprop(model.parameters(), lr=initial_lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model=model,\n",
    "    dataloader=dataloader,\n",
    "    device=device,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    epochs=epochs,\n",
    "    warmup_steps=warmup_steps,\n",
    "    initial_lr=initial_lr,\n",
    "    base_lr=base_lr,\n",
    "):\n",
    "    model = model.to(device=device)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        # if epoch < warmup_steps:\n",
    "        #     lr = initial_lr + (base_lr - initial_lr) * (epoch / warmup_steps)\n",
    "        #     for param_group in optimizer.param_groups:\n",
    "        #         param_group[\"lr\"] = lr\n",
    "        # else:\n",
    "        #     scheduler.step()\n",
    "        #     lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "        batch_loss = 0.0\n",
    "        for batch_inputs, batch_targets in dataloader:\n",
    "            batch_inputs = batch_inputs.to(device)\n",
    "            batch_targets = batch_targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_inputs)\n",
    "            loss = criterion(outputs, batch_targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_loss += loss.item()\n",
    "\n",
    "        avg_loss = batch_loss / len(dataloader)\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{epochs}, Cross Entropy Loss: {avg_loss:.4f}, Learning Rate: {initial_lr:.6f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Cross Entropy Loss: 1.3857, Learning Rate: 0.000100\n",
      "Epoch 2/20, Cross Entropy Loss: 1.2740, Learning Rate: 0.000100\n",
      "Epoch 3/20, Cross Entropy Loss: 1.2146, Learning Rate: 0.000100\n",
      "Epoch 4/20, Cross Entropy Loss: 1.1670, Learning Rate: 0.000100\n",
      "Epoch 5/20, Cross Entropy Loss: 1.1250, Learning Rate: 0.000100\n",
      "Epoch 6/20, Cross Entropy Loss: 1.0930, Learning Rate: 0.000100\n",
      "Epoch 7/20, Cross Entropy Loss: 1.0601, Learning Rate: 0.000100\n",
      "Epoch 8/20, Cross Entropy Loss: 1.0325, Learning Rate: 0.000100\n",
      "Epoch 9/20, Cross Entropy Loss: 1.0116, Learning Rate: 0.000100\n",
      "Epoch 10/20, Cross Entropy Loss: 0.9888, Learning Rate: 0.000100\n",
      "Epoch 11/20, Cross Entropy Loss: 0.9638, Learning Rate: 0.000100\n",
      "Epoch 12/20, Cross Entropy Loss: 0.9446, Learning Rate: 0.000100\n",
      "Epoch 13/20, Cross Entropy Loss: 0.9259, Learning Rate: 0.000100\n",
      "Epoch 14/20, Cross Entropy Loss: 0.9065, Learning Rate: 0.000100\n",
      "Epoch 15/20, Cross Entropy Loss: 0.8914, Learning Rate: 0.000100\n",
      "Epoch 16/20, Cross Entropy Loss: 0.8669, Learning Rate: 0.000100\n",
      "Epoch 17/20, Cross Entropy Loss: 0.8501, Learning Rate: 0.000100\n",
      "Epoch 18/20, Cross Entropy Loss: 0.8370, Learning Rate: 0.000100\n",
      "Epoch 19/20, Cross Entropy Loss: 0.8218, Learning Rate: 0.000100\n",
      "Epoch 20/20, Cross Entropy Loss: 0.8080, Learning Rate: 0.000100\n"
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Accuracy: 65.67%\n",
      "Accuracy of class 0: 63.41%\n",
      "Accuracy of class 1: 73.40%\n",
      "Accuracy of class 2: 71.53%\n",
      "Accuracy of class 3: 62.35%\n",
      "Accuracy of class 4: 33.05%\n",
      "Accuracy of class 5: 70.66%\n"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy(model, dataloader, num_classes):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    class_correct = torch.zeros(num_classes)\n",
    "    class_samples = torch.zeros(num_classes)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, predictions = torch.max(\n",
    "                outputs, 1\n",
    "            )\n",
    "\n",
    "            total_correct += (predictions == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "            for i in range(len(labels)):\n",
    "                label = labels[i]\n",
    "                if predictions[i] == label:\n",
    "                    class_correct[label] += 1\n",
    "                class_samples[label] += 1\n",
    "\n",
    "    total_accuracy = total_correct / total_samples * 100\n",
    "    class_accuracy = class_correct / class_samples * 100\n",
    "\n",
    "    return total_accuracy, class_accuracy\n",
    "\n",
    "\n",
    "num_classes = 6\n",
    "total_accuracy, class_accuracy = calculate_accuracy(\n",
    "    model, num_classes=6, dataloader=test_dataloader\n",
    ")\n",
    "\n",
    "print(f\"Total Accuracy: {total_accuracy:.2f}%\")\n",
    "for i in range(num_classes):\n",
    "    print(f\"Accuracy of class {i}: {class_accuracy[i]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "\n",
    "# test_dataset = TensorDataset(test_features, test_labels)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=6).to(device)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for batch_test_inputs, batch_test_targets in test_dataloader:\n",
    "#         outputs = model(batch_test_inputs)\n",
    "#         predictions = torch.argmax(outputs, dim=1)\n",
    "#         accuracy.update(predictions, batch_test_targets)\n",
    "\n",
    "# final_accuracy = accuracy.compute().item()\n",
    "# print(f\"Accuracy on the test data: {final_accuracy * 100:.2f}%\")\n",
    "\n",
    "# accuracy.reset()\n",
    "\n",
    "\n",
    "# def lr_finder(model, criterion, optimizer, train_loader, init_value=1e-10, final_value=10, beta=0.98):\n",
    "#     num = len(train_loader) - 1\n",
    "#     mult = (final_value / init_value) ** (1/num)\n",
    "#     lr = init_value\n",
    "#     optimizer.param_groups[0]['lr'] = lr\n",
    "#     avg_loss, best_loss = 0.0, 0.0\n",
    "#     losses, log_lrs = [], []\n",
    "\n",
    "#     for i, (inputs, labels) in enumerate(train_loader):\n",
    "#         # Move data to the correct device\n",
    "#         inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "#         # Zero the gradients\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs, labels)\n",
    "\n",
    "#         # Compute the smoothed loss\n",
    "#         avg_loss = beta * avg_loss + (1 - beta) * loss.item()\n",
    "#         smoothed_loss = avg_loss / (1 - beta ** (i + 1))\n",
    "\n",
    "#         # Stop if the loss is exploding\n",
    "#         if i > 1 and smoothed_loss > 4 * best_loss:\n",
    "#             break\n",
    "\n",
    "#         # Record the best loss\n",
    "#         if smoothed_loss < best_loss or i == 0:\n",
    "#             best_loss = smoothed_loss\n",
    "\n",
    "#         # Backward pass and update the weights\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Store the values\n",
    "#         losses.append(smoothed_loss)\n",
    "#         log_lrs.append(lr)\n",
    "\n",
    "#         # Update the learning rate\n",
    "#         lr *= mult\n",
    "#         optimizer.param_groups[0]['lr'] = lr\n",
    "\n",
    "#     # Plot the learning rate vs loss graph\n",
    "#     plt.plot(log_lrs, losses)\n",
    "#     plt.xscale('log')\n",
    "#     plt.xlabel(\"Learning Rate\")\n",
    "#     plt.ylabel(\"Loss\")\n",
    "#     plt.show()\n",
    "\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-8)\n",
    "\n",
    "# lr_finder(model, criterion, optimizer, dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a1_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
