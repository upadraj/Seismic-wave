{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import networkx as nx\n",
    "from pathlib import Path\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import Accuracy\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import to_networkx\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.nn import GATConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['..\\\\..\\\\data\\\\processed\\\\balance_16k\\\\preprocessing\\\\x_train.csv', '..\\\\..\\\\data\\\\processed\\\\balance_16k\\\\preprocessing\\\\y_train.csv']\n",
      "['S', 'Pn', 'Lg', 'Pg', 'Sn', 'P']\n"
     ]
    }
   ],
   "source": [
    "# path = f\"data\\processed\\balance_16k\\preprocessing\"\n",
    "path = Path(\"./../../data/processed/balance_16k/preprocessing\")\n",
    "input_paths = [\n",
    "    os.path.join(path, value) for i, value in enumerate(sorted(os.listdir(path)))\n",
    "]\n",
    "print(input_paths)\n",
    "dataframes = [pd.read_csv(file, header=None) for file in input_paths]\n",
    "int_codes, str_label = pd.factorize(dataframes[1][0])\n",
    "\n",
    "# X = dataframes[0].iloc[:500]\n",
    "X = dataframes[0]\n",
    "X = X.apply(lambda x: x- x.mean()/ x.std())\n",
    "# y = int_codes[:500]\n",
    "y = int_codes\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=22\n",
    ")\n",
    "station_list = list(pd.unique(dataframes[1][0]))\n",
    "print(station_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[50, 144], edge_index=[2, 1225], y=4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_graph_for_gnn(X, y, num_segments=50, overlap=0.5):\n",
    "    graphs = []\n",
    "    segment_size = X.shape[1] // num_segments\n",
    "    overlap_size = int(segment_size * overlap)\n",
    "\n",
    "    for row in range(X.shape[0]):\n",
    "        G = nx.Graph()\n",
    "\n",
    "        for node_idx in range(num_segments):\n",
    "            start = node_idx * (segment_size - overlap_size)\n",
    "            end = start + segment_size\n",
    "            node_features = X.iloc[row][start:end]\n",
    "            G.add_node(node_idx, features=node_features)\n",
    "\n",
    "        for i in range(num_segments):\n",
    "            for j in range(i + 1, num_segments):\n",
    "                G.add_edge(i, j)\n",
    "\n",
    "        edge_index = torch.tensor(list(G.edges)).t().contiguous()\n",
    "        x = torch.tensor(\n",
    "            [G.nodes[node][\"features\"] for node in G.nodes()], dtype=torch.float\n",
    "        )\n",
    "\n",
    "        graph_data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "        graph_data.y = torch.tensor(y[row], dtype=torch.long)\n",
    "\n",
    "        graphs.append(graph_data)\n",
    "\n",
    "    return graphs\n",
    "\n",
    "graph_train_data = create_graph_for_gnn(X_train, y_train, overlap=0.5)  # You can adjust overlap as needed\n",
    "graph_test_data = create_graph_for_gnn(X_test, y_test, overlap=0.5)\n",
    "\n",
    "torch.save(graph_train_data, 'graph_train_data.pth')\n",
    "torch.save(graph_test_data, 'graph_test_data.pth')\n",
    "\n",
    "graph_train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggreration type as (aggr)\n",
    "\n",
    "epochs = 10, training = 425 samples\n",
    "| Aggregation types| Accuracy|\n",
    "|-------------------|---------|\n",
    "|    'mean'         | 36      |\n",
    "|    'sum' or 'add' | 36      |\n",
    "|   'max'           | 23      |\n",
    "|   'min'           | 23      |\n",
    "| 'median'          | 35      |\n",
    "|   'mul'           | 24      |\n",
    "|   'std'           | 36      |\n",
    "|  'var'            | 36      |\n",
    "| 'softmax'         | 36      |\n",
    "\n",
    "When run on all the sample except testing accuracy is around 20% on average for all of these aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\incognito\\AppData\\Local\\Temp\\ipykernel_15948\\3439482685.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_train_data = torch.load('graph_train_data.pth')\n",
      "C:\\Users\\incognito\\AppData\\Local\\Temp\\ipykernel_15948\\3439482685.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_test_data = torch.load('graph_test_data.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 37.8123,\n",
      "Epoch 2, Train Loss: 39.0867,\n",
      "Epoch 3, Train Loss: 38.9955,\n",
      "Epoch 4, Train Loss: 38.4136,\n",
      "Epoch 5, Train Loss: 39.3020,\n",
      "Epoch 6, Train Loss: 38.0623,\n",
      "Epoch 7, Train Loss: 38.0346,\n",
      "Epoch 8, Train Loss: 37.9839,\n",
      "Epoch 9, Train Loss: 37.9068,\n",
      "0.18119497763024967\n",
      "tensor(0.1812, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, aggr='sum'):\n",
    "        super(GCN, self, ).__init__()\n",
    "\n",
    "        self.conv1 = GCNConv(144, 128, aggr)\n",
    "        self.conv2 = GCNConv(128, 64, aggr)\n",
    "        self.fc = torch.nn.Linear(3200, 6)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.tanh(self.conv1(x, edge_index))\n",
    "        x = F.tanh(self.conv2(x,edge_index))\n",
    "        x = x.view(data.num_graphs, -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GCN().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "graph_train_data = torch.load('graph_train_data.pth')\n",
    "graph_test_data = torch.load('graph_test_data.pth')\n",
    "\n",
    "train_loader = DataLoader(graph_train_data, shuffle=True)\n",
    "test_loader = DataLoader(graph_test_data, shuffle=False)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "for epoch in range(1, 10):\n",
    "    train_loss = train()\n",
    "    print(f\"Epoch {epoch}, Train Loss: {train_loss:.4f},\")\n",
    "\n",
    "def evaluate(train_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    acc = 0\n",
    "    accuracy_metric = Accuracy(task=\"multiclass\", num_classes=6).to(device)\n",
    "    accuracy_metric.reset()\n",
    "    with torch.no_grad():\n",
    "        for data in train_loader:\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output, dim=1)\n",
    "            # print(f\"Predicted: {predicted}, Actual: {data.y}\")\n",
    "            correct += (predicted == data.y).sum().item()\n",
    "            total += data.y.size(0)\n",
    "            acc += accuracy_metric(predicted, data.y)\n",
    "\n",
    "    return correct / total, acc / len(train_loader)\n",
    "\n",
    "\n",
    "correct, accu = evaluate(train_loader)\n",
    "print(correct)\n",
    "print(accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_graph(graph_data, title=\"Graph Visualization\"):\n",
    "#     \"\"\"\n",
    "#     Plots the graph created for the GNN.\n",
    "\n",
    "#     Parameters:\n",
    "#     - graph_data: A PyTorch Geometric Data object representing the graph.\n",
    "#     - title: Title of the plot.\n",
    "#     \"\"\"\n",
    "#     # Convert back to networkx for visualization\n",
    "#     G = nx.Graph()\n",
    "\n",
    "#     # Add nodes with features directly from the PyTorch tensor\n",
    "#     node_features = graph_data.x  # Stay as a tensor\n",
    "#     for i, features in enumerate(node_features):\n",
    "#         G.add_node(i, features=features.tolist())  # Convert each feature to list\n",
    "\n",
    "\n",
    "#     edge_index = graph_data.edge_index.t()\n",
    "#     for edge in edge_index:\n",
    "#         G.add_edge(int(edge[0]), int(edge[1]))\n",
    "\n",
    "#     pos = nx.spring_layout(G)\n",
    "\n",
    "#     plt.figure(figsize=(8, 6))\n",
    "\n",
    "#     nx.draw(G, pos, with_labels=True, node_size=500, node_color=\"lightblue\", font_size=10, font_weight=\"bold\")\n",
    "\n",
    "#     node_labels = {i: f\"{i}\" for i in G.nodes()}\n",
    "#     nx.draw_networkx_labels(G, pos, labels=node_labels)\n",
    "\n",
    "#     plt.title(title)\n",
    "#     plt.show()\n",
    "\n",
    "# plot_graph(graph_train_data[20], title=\"Sample Graph Visualization\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a1_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
