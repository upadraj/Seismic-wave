{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import degree\n",
    "\n",
    "class GCNConvPearson(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GCNConvPearson, self).__init__(aggr='add')  # We will manually handle aggregation\n",
    "        self.weight = torch.nn.Parameter(torch.Tensor(in_channels, out_channels))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.weight)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        # Compute Pearson correlation\n",
    "        edge_index_i, edge_index_j = edge_index[0], edge_index[1]  # Node i to j\n",
    "        \n",
    "        # Calculate mean of node features for each node\n",
    "        x_mean = torch.mean(x, dim=1, keepdim=True)\n",
    "        \n",
    "        # Subtract the mean\n",
    "        x_centered = x - x_mean\n",
    "\n",
    "        # Message passing (to neighbors)\n",
    "        return self.propagate(edge_index, x=x_centered, norm=x_mean)\n",
    "    \n",
    "    def message(self, x_j, x_i, norm):\n",
    "        # Pearson correlation numerator: sum((x_i - mean(x_i)) * (x_j - mean(x_j)))\n",
    "        numerator = torch.sum(x_i * x_j, dim=-1, keepdim=True)\n",
    "\n",
    "        # Denominator: sqrt(sum((x_i - mean(x_i))^2) * sum((x_j - mean(x_j))^2))\n",
    "        x_i_norm = torch.sqrt(torch.sum(x_i**2, dim=-1, keepdim=True))\n",
    "        x_j_norm = torch.sqrt(torch.sum(x_j**2, dim=-1, keepdim=True))\n",
    "        \n",
    "        denominator = x_i_norm * x_j_norm + 1e-7  # Adding a small value to avoid division by zero\n",
    "\n",
    "        # Pearson correlation as weight\n",
    "        pearson_corr = numerator / denominator\n",
    "\n",
    "        # Weight the features of the neighbors by the correlation coefficient\n",
    "        return pearson_corr * x_j\n",
    "    \n",
    "    def update(self, aggr_out):\n",
    "        # Apply the learned weights to the aggregated feature\n",
    "        return aggr_out @ self.weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Custom GCN with Pearson Correlation and Global Pooling for Graph-Level Prediction\n",
    "class GCNWithPearsonAggregationGraphLevel(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GCNWithPearsonAggregationGraphLevel, self).__init__()\n",
    "        self.conv1 = GCNConvPearson(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConvPearson(hidden_channels, hidden_channels)\n",
    "        self.fc = torch.nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # First GCN layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Second GCN layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        # Pooling layer (global mean pooling to aggregate node features into graph-level representation)\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "        \n",
    "        # Fully connected layer for final prediction (graph-level)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "\n",
    "# Create a batch of fully connected graphs, each with 50 nodes and 144 features\n",
    "num_nodes = 50\n",
    "num_features = 144\n",
    "num_classes = 6\n",
    "graph_train_data = torch.load('graph_train_data.pth')\n",
    "graph_test_data = torch.load('graph_test_data.pth')\n",
    "\n",
    "\n",
    "# Create a batch of graphs\n",
    "train_batch = Batch.from_data_list(graph_train_data)\n",
    "test_batch = Batch.from_data_list(graph_test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, optimizer, and loss function for graph-level prediction\n",
    "model = GCNWithPearsonAggregationGraphLevel(in_channels=num_features, hidden_channels=64, out_channels=num_classes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop for graph-level prediction\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass: use batch data for graph-level prediction\n",
    "    out = model(train_batch.x, train_batch.edge_index, train_batch.batch)\n",
    "    \n",
    "    # Compute loss using graph-level labels\n",
    "    loss = loss_fn(out, train_batch.y)  # batch.y contains the labels for the entire graphs\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # if epoch % 10 == 0:\n",
    "    print(f'Epoch {epoch}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_accuracy(pred, target):\n",
    "    \"\"\"Calculate accuracy by comparing predicted and true labels.\"\"\"\n",
    "    correct = pred.eq(target).sum().item()\n",
    "    accuracy = correct / target.size(0)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "model.eval()\n",
    "output = model(test_batch.x, test_batch.edge_index, test_batch.batch)\n",
    "predicted_classes = output.argmax(dim=1) \n",
    "\n",
    "final_accuracy = calculate_accuracy(predicted_classes, test_batch.y)\n",
    "print(f'Final Accuracy Testing: {final_accuracy:.4f}')\n",
    "\n",
    "\n",
    "output = model(train_batch.x, train_batch.edge_index, train_batch.batch)\n",
    "predicted_classes = output.argmax(dim=1) \n",
    "final_accuracy = calculate_accuracy(predicted_classes, test_batch.y)\n",
    "print(f'Final Accuracy Testing: {final_accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a1_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
